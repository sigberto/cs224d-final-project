{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from qa_model import Encoder, QASystem, Decoder\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.01, \"Learning rate.\")\n",
    "tf.app.flags.DEFINE_float(\"max_gradient_norm\", 10.0, \"Clip gradients to this norm.\")\n",
    "tf.app.flags.DEFINE_float(\"dropout\", 0.15, \"Fraction of units randomly dropped on non-recurrent connections.\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 10, \"Batch size to use during training.\")\n",
    "tf.app.flags.DEFINE_integer(\"epochs\", 10, \"Number of epochs to train.\")\n",
    "tf.app.flags.DEFINE_integer(\"state_size\", 200, \"Size of each model layer.\")\n",
    "tf.app.flags.DEFINE_integer(\"output_size\", 750, \"The output size of your model.\")\n",
    "tf.app.flags.DEFINE_integer(\"embedding_size\", 100, \"Size of the pretrained vocabulary.\")\n",
    "tf.app.flags.DEFINE_string(\"data_dir\", \"data/squad\", \"SQuAD directory (default ./data/squad)\")\n",
    "tf.app.flags.DEFINE_string(\"train_dir\", \"train\", \"Training directory to save the model parameters (default: ./train).\")\n",
    "tf.app.flags.DEFINE_string(\"load_train_dir\", \"\", \"Training directory to load model parameters from to resume training (default: {train_dir}).\")\n",
    "tf.app.flags.DEFINE_string(\"log_dir\", \"log\", \"Path to store log and flag files (default: ./log)\")\n",
    "tf.app.flags.DEFINE_string(\"optimizer\", \"adam\", \"adam / sgd\")\n",
    "tf.app.flags.DEFINE_integer(\"print_every\", 1, \"How many iterations to do per print.\")\n",
    "tf.app.flags.DEFINE_integer(\"keep\", 0, \"How many checkpoints to keep, 0 indicates keep all.\")\n",
    "tf.app.flags.DEFINE_string(\"vocab_path\", \"data/squad/vocab.dat\", \"Path to vocab file (default: ./data/squad/vocab.dat)\")\n",
    "tf.app.flags.DEFINE_string(\"embed_path\", \"\", \"Path to the trimmed GLoVe embedding (default: ./data/squad/glove.trimmed.{embedding_size}.npz)\")\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "\n",
    "def initialize_model(session, model, train_dir):\n",
    "    ckpt = tf.train.get_checkpoint_state(train_dir)\n",
    "    v2_path = ckpt.model_checkpoint_path + \".index\" if ckpt else \"\"\n",
    "    if ckpt and (tf.gfile.Exists(ckpt.model_checkpoint_path) or tf.gfile.Exists(v2_path)):\n",
    "        logging.info(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        logging.info(\"Created model with fresh parameters.\")\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        logging.info('Num params: %d' % sum(v.get_shape().num_elements() for v in tf.trainable_variables()))\n",
    "    return model\n",
    "\n",
    "\n",
    "def initialize_vocab(vocab_path):\n",
    "    if tf.gfile.Exists(vocab_path):\n",
    "        rev_vocab = []\n",
    "        with tf.gfile.GFile(vocab_path, mode=\"rb\") as f:\n",
    "            rev_vocab.extend(f.readlines())\n",
    "        rev_vocab = [line.strip('\\n') for line in rev_vocab]\n",
    "        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
    "        return vocab, rev_vocab\n",
    "    else:\n",
    "        raise ValueError(\"Vocabulary file %s not found.\", vocab_path)\n",
    "\n",
    "\n",
    "def get_normalized_train_dir(train_dir):\n",
    "    \"\"\"\n",
    "    Adds symlink to {train_dir} from /tmp/cs224n-squad-train to canonicalize the\n",
    "    file paths saved in the checkpoint. This allows the model to be reloaded even\n",
    "    if the location of the checkpoint files has moved, allowing usage with CodaLab.\n",
    "    This must be done on both train.py and qa_answer.py in order to work.\n",
    "    \"\"\"\n",
    "    global_train_dir = '/tmp/cs224n-squad-train'\n",
    "    if os.path.exists(global_train_dir):\n",
    "        os.unlink(global_train_dir)\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.makedirs(train_dir)\n",
    "    os.symlink(os.path.abspath(train_dir), global_train_dir)\n",
    "    return global_train_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contextPath = pjoin(FLAGS.data_dir, \"train.ids.context\")\n",
    "questionPath = pjoin(FLAGS.data_dir, \"train.ids.question\")\n",
    "answerPath = pjoin(FLAGS.data_dir, \"train.span\")\n",
    "\n",
    "context = open(contextPath).read().split('\\n')\n",
    "contextData = [[int(x) for x in c.split(' ')] for c in context[0:-1]]\n",
    "\n",
    "question = open(questionPath).read().split('\\n')\n",
    "questionData = [[int(x) for x in q.split(' ')] for q in question[0:-1]]\n",
    "\n",
    "answer = open(answerPath).read().split('\\n')\n",
    "answerData = [[int(x) for x in a.split(' ')] for a in answer[0:-1]]\n",
    "\n",
    "dataset = (contextData, questionData, answerData)\n",
    "\n",
    "embed_path = FLAGS.embed_path or pjoin(\"data\", \"squad\", \"glove.trimmed.{}.npz\".format(FLAGS.embedding_size))\n",
    "vocab_path = FLAGS.vocab_path or pjoin(FLAGS.data_dir, \"vocab.dat\")\n",
    "vocab_path = '../' + vocab_path\n",
    "vocab, rev_vocab = initialize_vocab(vocab_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a5df1ffb053e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0mload_train_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_normalized_train_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_train_dir\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0minitialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_train_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'qa' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "        load_train_dir = get_normalized_train_dir(FLAGS.load_train_dir or FLAGS.train_dir)\n",
    "        initialize_model(sess, qa, load_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '../data/squad/train.question'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
